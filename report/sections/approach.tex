\section{Approach}
In this part, we will employ several unconstrained and constrained algorithms for solving minimum surface and obstacle problems while we also introduce four acceleration techniques to enhance our experiment results and performance.

\subsection{Unconstrained Optimization Algorithms}
The gradient descent method with backtracking is the most common and basic algorithm to solve unconstrained optimization problem. The full gradient method is showed in algorithm \ref{alg:Gradient}. The step size in every iteration should satisfy the Amrijo Condition:
$$
f\left(x^{k}+\alpha_{k} d^{k}\right)-f\left(x^{k}\right) \leq \gamma \alpha_{k} \cdot \nabla f\left(x^{k}\right)^{\top} d^{k}
$$
where $\alpha_{k}$ can be determined after finitely many steps if $d^{k}$ is a descent direction.
\begin{algorithm}[H]
  \caption{Gradient Descent Method (with Backtracking)}
  \label{alg:Gradient}
  \begin{algorithmic}[1]
  \REQUIRE
  Initial point $x^{0}$
  \STATE For $k=0,1, \ldots$ \\
  \STATE Choose stepsize $\alpha^{k}$ by backtracking. \\
  \STATE $x^{k+1}=x^{k}-\alpha^{k} \nabla f\left(x^{k}\right)$. \\
  \STATE Stop if $\left\|\nabla f\left(x^{k+1}\right)\right\| \leq \epsilon$. \\     
  \ENSURE $x^{k+1}$ 
 \end{algorithmic}
 \end{algorithm} 
 
 Globalized newton method have better convergence performance compared with pure newton method, as represented in algorithm \ref{alg:Newton}. It shows that the whole sequence $\left(x^{k}\right)_{k}$ converges q-superlinearly to $x^{*}$. In particular, if $\nabla^{2} f$ is Lipschitz continuous in a neighborhood of $x^{*}$, the rate of convergence is q-quadratic. 
 \begin{algorithm}[H]
   \caption{Globalized Newton Method}
   \label{alg:Newton}
   \begin{algorithmic}[1]
   \REQUIRE
   Initial point $x^{0}, \beta_{1}, \beta_{2}, p$
   \STATE For $k=0,1, \ldots$ \\
   \STATE Compute the Newton direction $s^{k}$ by solving $\nabla^{2} f\left(x^{k}\right) s^{k}=-\nabla f\left(x^{k}\right)$. \\
   \STATE If $-\nabla f\left(x^{k}\right)^{\mathrm{T}} s^{k} \geq \min \left\{\beta_{1}, \beta_{2}\left\|s^{k}\right\|^{p}\right\}\left\|s^{k}\right\|^{2}$, then set $d^{k}=s^{k}$. Otherwise set $d^{k}=-\nabla f\left(x^{k}\right)$. \\
   \STATE Choose stepsize $\alpha^{k}$ by backtracking. \\  
   \STATE $x^{k+1}=x^{k}+\alpha^{k} d^{k}$. \\
   \STATE  Stop if $\left\|\nabla f\left(x^{k+1}\right)\right\| \leq \epsilon$. \\  
   \ENSURE $x^{k+1}$ 
  \end{algorithmic}
  \end{algorithm} 
 
  Since the BFGS update is widely used to solve general nonlinear minimization problems, the most of studies on limited memory methods are concentrate on the limited memory BFGS (L-BFGS) method \cite{zhang1999new}, which has better performance, showed in algorithm \ref{alg:L-BFGS} as below.
 
 \begin{algorithm}[H]
   \caption{L-BFGS Method}
   \label{alg:L-BFGS}
   \begin{algorithmic}[1]
   \REQUIRE
 Initial point $x^{0}, B_{0}$ \\
   \STATE For $k=0,1, \ldots$ \\
   \STATE Compute the quasi-Newton direction $d^{k}$ by solving $B_{k} s^{k}=-\nabla f\left(x^{k}\right), x^{k+1}=x^{k}+s^{k}$. \\
   \STATE $\quad$ Stop if $\left\|\nabla f\left(x^{k+1}\right)\right\| \leq \epsilon$. \\
   \STATE Set $y^{k}=\nabla f\left(x^{k+1}\right)-\nabla f\left(x^{k}\right), B_{k+1}^{\text {BFGS }}=B_{k}+\frac{y^{k}\left(y^{k}\right)^{\top}}{\left(y^{k}\right)^{\top} s^{k}}-\frac{\left(B_{k} s^{k}\right)\left(B_{k} s^{k}\right)^{\top}}{\left(s^{k}\right)^{\top} B_{k} s^{k}}$ \\
   \ENSURE $x^{k+1}$ \\
   \end{algorithmic}
 \end{algorithm}
 
\subsection{Constrained Optimization Algorithms}
The penalty method is a classical approach for constrained optimization problem where the unconstrained problem are built by adding penalty terms for the constraints to the objective function. It proves that the larger the penalty parameter $\alpha$, the better we will approximate the initial constrained problem. However, the numerical performance of the penalty subproblems becomes worse with larger penalty parameter. In each iteration, we can apply typical unconstrained optimization method to solve this problem, as illustrated in algorithm \ref{alg:Penalty}.
\begin{algorithm}[H]
  \caption{Quadratic Penalty Method}
  \label{alg:Penalty}
  \begin{algorithmic}[1]
  \REQUIRE
  Initial point $x^{-1}$, penalty parameter $\alpha_{0}>0$
  \STATE For $k=0,1, \ldots$ \\
  \STATE Compute the global solution $x^{k}$ of the penalty problem $\min _{x} P_{\alpha_{k}}(x)$. \\     
  \ENSURE Terminate if $x^{k} \in X$. Otherwise select $\alpha_{k+1}>\alpha_{k}$.  
 \end{algorithmic}
 \end{algorithm} 

The projected gradient method is similar to the gradient descent method where we introduce the projection operator to guarantee the optimal solution belonging to the feasible sets. The full process can be seen in 
algorithm \ref{alg:Projected}.
 \begin{algorithm}[H]
  \caption{Projected Gradient Method}
  \label{alg:Projected}
  \begin{algorithmic}[1]
  \REQUIRE
  Initial point $x^{0} \in C, \sigma, \gamma \in(0,1)$
  \STATE For $k=0,1, \ldots$ \\
  \STATE $\quad$ Select $\lambda_{k}>0$ and compute $\nabla f\left(x^{k}\right)$ and the new direction $d^{k}=-F_{\lambda_{k}}\left(x^{k}\right)$.  \\
  \STATE $\quad$ If $\left\|d^{k}\right\| \leq \lambda_{k} \epsilon$, then STOP and $x^{k}$ is output.  \\
  \STATE Choose a maximal step size $\alpha_{k}$ by backtracking. \\    
  \STATE Set $x^{k+1}=x^{k}+\alpha_{k} d^{k}$
  %\ENSURE $x^{k+1}$ 
 \end{algorithmic}
 \end{algorithm} 



\subsection{Addtional Techniques}
In our experiment, we will explore three additional techniques (nonmonotone line search procedures, Barzilai-Borwein steps, inertial techniques and momentum) to adjust and potentially accelerate our proposed gradient descent algorithm. 

Exact line search is proposed to modify the steps when comparing with gradient method with backtracking. Generally, it's expected to have better performance but with poor operation time. It can be formulated as $\alpha_{k}=\operatorname{argmin}_{\alpha \geq 0} f\left(x^{k}+\alpha d^{k}\right)$ to choose $\alpha_{k}$ to achieve the largest descent. 

Inertial techniques and momentum method that are similar to Nesterovâ€™s acceleration behave well to improve the convergence performance. The extrapolation step can be represented as $$
y^{k+1}=x^{k}+\beta_{k}\left(x^{k}-x^{k-1}\right), \quad \beta_{k}>0
$$
The new iterate $x^{k+1}$ is updated as follows:
$$
x^{k+1}=y^{k+1}-\alpha_{k} \nabla f\left(x^{k}\right)=x^{k}-\alpha_{k} \nabla f\left(x^{k}\right)+\beta_{k}\left(x^{k}-x^{k-1}\right)
$$
where $\alpha_{k}$ is a suitable step size and the gradient is now evaluated at $x^{k}$ and not at $y^{k+1} $.

The Barzilai-Borwein (BB) method is a popular and efficient tool for solving large-scale unconstrained optimization problems \cite{raydan1997barzilai}. Its search direction is the same as for the steepest descent method, but its stepsize rule is different. Owing to this, it converges much faster than the steepest descent method \cite{burdakov2019stabilized}. The full implement process is showed in algorithm \ref{alg:BB method} as follows.

\begin{algorithm}[H]
  \caption{Barzilai and Borwein Gradient Method}
  \label{alg:BB method}
  \begin{algorithmic}[1]
  \REQUIRE
Initial point $x^{0},0<\varepsilon \ll 1 $ \\
  \STATE For $k=0,1, \ldots$ \\
  \STATE If $\left\|\nabla f\left(x^{k}\right)\right\| \leq \varepsilon,$ stop ; otherwise let $d_{k}=-\nabla f\left(x^{k}\right)$. \\
  \STATE If $k=0,$ find $\alpha_{0}$ by line search; otherwise compute $\alpha_{k}$ by 
  $
  \alpha_{k}=\frac{s_{k-1}^{T} y_{k-1}}{y_{k-1}^{T} y_{k-1}}
  $
  where $s_{k-1}=x_{k}-x_{k-1}, y_{k-1}=\nabla f\left(x^{k}\right)-\nabla f\left(x^{k-1}\right)$. \\
  \STATE Set $x_{k+1}=x_{k}+\alpha_{k} d_{k}$ \\
  \STATE $k:=k+1$, return to Step 1 \\
  \end{algorithmic}
\end{algorithm}





